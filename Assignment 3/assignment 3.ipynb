{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cody Nichols' Assignment 3 COMP 6970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class BatchNorm1dManual(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, gamma, beta, eps=1e-5):\n",
    "        # Step 1: Compute mean and variance\n",
    "        mu = x.mean(dim=0)\n",
    "        var = x.var(dim=0, unbiased=False)\n",
    "\n",
    "        # Step 2: Normalize the inputs\n",
    "        x_hat = (x - mu) / torch.sqrt(var + eps)\n",
    "        \n",
    "        # Step 3: Apply scale and shift\n",
    "        result = gamma * x_hat + beta\n",
    "\n",
    "        # Step 4: Save computed values for backward pass\n",
    "        ctx.save_for_backward(x, mu, var, x_hat, gamma, beta, torch.tensor(eps))\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout):\n",
    "        # Step 1: Obtain saved variables in forward\n",
    "        x, mu, var, x_hat, gamma, beta, eps = ctx.saved_tensors\n",
    "        N, D = x.shape\n",
    "\n",
    "        # Step 2: Gradients for gamma and beta\n",
    "        dgamma = torch.sum(dout * x_hat, dim=0)\n",
    "        dbeta = torch.sum(dout, dim=0)\n",
    "\n",
    "        # Step 3: Backprop through the normalization\n",
    "        dx_hat = dout * gamma\n",
    "\n",
    "        # Step 4: Gradient with respect to input\n",
    "        std_inv = 1.0 / torch.sqrt(var + eps)\n",
    "        dx = ((1.0 / N) * std_inv) * (N * dx_hat - torch.sum(dx_hat, dim=0) - x_hat * torch.sum(dx_hat * x_hat, dim=0))\n",
    "\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "N, D = 5, 4\n",
    "# Create random inputs\n",
    "x = torch.randn(N, D, dtype=torch.double, requires_grad=True)\n",
    "gamma = torch.ones(D, dtype=torch.double, requires_grad=True)\n",
    "beta = torch.zeros(D, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Wrap inputs in tuple and perform gradcheck\n",
    "input_tuple = (x, gamma, beta)\n",
    "grad_check = gradcheck(BatchNorm1dManual.apply, input_tuple, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check passed:\", grad_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class InstanceNorm2d(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, gamma, beta, eps=1e-5):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # Reshape x to compute mean and variance over H and W\n",
    "        x_reshaped = x.view(N, C, -1)\n",
    "\n",
    "        # Compute mean and variance per instance per channel\n",
    "        mu = x_reshaped.mean(dim=2, keepdim=True)\n",
    "        var = x_reshaped.var(dim=2, unbiased=False, keepdim=True)\n",
    "\n",
    "        # Normalize\n",
    "        x_hat = (x_reshaped - mu) / torch.sqrt(var + eps)\n",
    "\n",
    "        # Reshape x_hat back\n",
    "        x_hat = x_hat.view(N, C, H, W)\n",
    "\n",
    "        # Scale and shift\n",
    "        gamma = gamma.view(1, C, 1, 1)\n",
    "        beta = beta.view(1, C, 1, 1)\n",
    "        result = gamma * x_hat + beta\n",
    "\n",
    "        # Save variables for backward pass\n",
    "        ctx.save_for_backward(x_hat, var, gamma, torch.tensor(eps))\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved values\n",
    "        x_hat, var, gamma, eps = ctx.saved_tensors\n",
    "        N, C, H, W = grad_output.shape\n",
    "        M = H * W\n",
    "\n",
    "        # Reshape tensors\n",
    "        x_hat = x_hat.view(N, C, -1)\n",
    "        grad_output = grad_output.view(N, C, -1)\n",
    "\n",
    "        # Gradients w.r.t. scale (gamma) and shift (beta)\n",
    "        dgamma = (grad_output * x_hat).sum(dim=(0, 2))\n",
    "        dbeta = grad_output.sum(dim=(0, 2))\n",
    "\n",
    "        # Reshape gamma\n",
    "        gamma = gamma.view(1, C, 1)\n",
    "\n",
    "        # Compute dx_hat\n",
    "        dx_hat = grad_output * gamma\n",
    "\n",
    "        # Compute for dx\n",
    "        std_inv = 1.0 / torch.sqrt(var + eps)\n",
    "\n",
    "        # Sum over spatial dimensions\n",
    "        dx_hat_sum = dx_hat.sum(dim=2, keepdim=True)\n",
    "        x_hat_dx_hat_sum = (dx_hat * x_hat).sum(dim=2, keepdim=True)\n",
    "\n",
    "        # Gradient w.r.t. input (x)\n",
    "        dx = (1.0 / M) * std_inv * (M * dx_hat - dx_hat_sum - x_hat * x_hat_dx_hat_sum)\n",
    "\n",
    "        # Reshape dx\n",
    "        dx = dx.view(N, C, H, W)\n",
    "\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "N, C, H, W = 2,3,4,4\n",
    "# Example usage\n",
    "x = torch.randn(N, C, H, W, dtype=torch.double, requires_grad=True)\n",
    "gamma = torch.ones(C, dtype=torch.double, requires_grad=True)\n",
    "beta = torch.zeros(C, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Wrap inputs in tuple and perform gradcheck\n",
    "input_tuple = (x, gamma, beta)\n",
    "grad_check = gradcheck(InstanceNorm2d.apply, input_tuple, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradient check passed:\", grad_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion is correct! The outputs match.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple model with Linear followed by BatchNorm1d\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.bn = nn.BatchNorm1d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "# Define a model that only uses the fused Linear layer (BatchNorm merged)\n",
    "class FusedModel(nn.Module):\n",
    "    def __init__(self, fused_layer):\n",
    "        super(FusedModel, self).__init__()\n",
    "        self.fc_fused = fused_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_fused(x)\n",
    "        return x\n",
    "\n",
    "# Function to test equivalence between original and fused models\n",
    "def test_fusion(original_model, fused_model, input_data):\n",
    "    # Get outputs from the original model\n",
    "    original_output = original_model(input_data)\n",
    "\n",
    "    # Get outputs from the fused model\n",
    "    fused_output = fused_model(input_data)\n",
    "\n",
    "    # Compare the outputs (element-wise)\n",
    "    if torch.allclose(original_output, fused_output, atol=1e-5):\n",
    "        print(\"Fusion is correct! The outputs match.\")\n",
    "    else:\n",
    "        print(\"Fusion is incorrect! The outputs do not match.\")\n",
    "\n",
    "# Fuse batch norm into the linear layer\n",
    "def fuse_batch_norm_into_linear(linear_layer, batch_norm_layer):\n",
    "    W = linear_layer.weight\n",
    "    b = linear_layer.bias\n",
    "\n",
    "    gamma = batch_norm_layer.weight\n",
    "    beta = batch_norm_layer.bias\n",
    "    running_mean = batch_norm_layer.running_mean\n",
    "    running_var = batch_norm_layer.running_var\n",
    "    eps = batch_norm_layer.eps\n",
    "\n",
    "    # Compute standard deviation\n",
    "    std = torch.sqrt(running_var + eps)\n",
    "\n",
    "    # Compute scale\n",
    "    scale = gamma / std\n",
    "\n",
    "    # Reshape scale\n",
    "    scale = scale.unsqueeze(1)\n",
    "\n",
    "    # Compute the weight\n",
    "    W_fused = scale * W\n",
    "\n",
    "    # Compute the bias\n",
    "    b_fused = scale.squeeze() * (b - running_mean) + beta\n",
    "\n",
    "    # Create a new Linear layer with fused parameters\n",
    "    fused_layer = nn.Linear(linear_layer.in_features, linear_layer.out_features)\n",
    "    fused_layer.weight = nn.Parameter(W_fused)\n",
    "    fused_layer.bias = nn.Parameter(b_fused)\n",
    "\n",
    "    return fused_layer\n",
    "\n",
    "# Define input size and output size\n",
    "input_size = 128\n",
    "output_size = 64\n",
    "\n",
    "# Instantiate the original model\n",
    "original_model = OriginalModel(input_size, output_size)\n",
    "\n",
    "# Switch the model to evaluation mode (important for BatchNorm)\n",
    "original_model.eval()\n",
    "\n",
    "# Generate some random input data\n",
    "input_data = torch.randn(10, input_size)\n",
    "\n",
    "# Forward pass through the original model\n",
    "with torch.no_grad():\n",
    "    original_output = original_model(input_data)\n",
    "\n",
    "# Fuse the BatchNorm layer into the linear layer\n",
    "fused_layer = fuse_batch_norm_into_linear(original_model.fc, original_model.bn)\n",
    "\n",
    "# Instantiate the fused model\n",
    "fused_model = FusedModel(fused_layer)\n",
    "\n",
    "# Switch the fused model to evaluation mode\n",
    "fused_model.eval()\n",
    "\n",
    "# Test if the original and fused models produce the same output\n",
    "test_fusion(original_model, fused_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Consistency Check for Linear Module: True\n",
      "Gradient Check for Linear Module: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias):\n",
    "        # Save tensors for backward pass\n",
    "        ctx.save_for_backward(x, weight, bias)\n",
    "        \n",
    "        # Perform linear transformation\n",
    "        y = x.mm(weight.t())\n",
    "        \n",
    "        # Add bias\n",
    "        y += bias.unsqueeze(0).expand_as(y)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout):\n",
    "        x, weight, bias = ctx.saved_tensors\n",
    "        # Gradient w.r.t. input\n",
    "        dx = dout.mm(weight)\n",
    "\n",
    "        # Gradient w.r.t. weight\n",
    "        dw = dout.t().mm(x)\n",
    "\n",
    "        # Gradient w.r.t. bias\n",
    "        db = dout.sum(dim=0)\n",
    "        \n",
    "        return dx, dw, db\n",
    "\n",
    "def gradient_check_and_output_check_linear():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Create input data\n",
    "    x = torch.randn(5, 3, dtype=torch.double, requires_grad=True)  # Input tensor (requires grad)\n",
    "    weight = torch.randn(2, 3, dtype=torch.double, requires_grad=True)  # Weight tensor\n",
    "    bias = torch.randn(2, dtype=torch.double, requires_grad=True)  # Bias tensor\n",
    "\n",
    "    # Custom Linear function\n",
    "    linear_custom = LinearFunction.apply\n",
    "    output_custom = linear_custom(x, weight, bias)\n",
    "\n",
    "    # PyTorch's built-in Linear layer\n",
    "    linear_torch = torch.nn.Linear(3, 2, bias=True).double()\n",
    "    linear_torch.weight = torch.nn.Parameter(weight.clone().detach())\n",
    "    linear_torch.bias = torch.nn.Parameter(bias.clone().detach())\n",
    "    output_torch = linear_torch(x)\n",
    "\n",
    "    # Compare outputs\n",
    "    print('Output Consistency Check for Linear Module:', torch.allclose(output_custom, output_torch, atol=1e-6))\n",
    "\n",
    "    # Use gradcheck on the custom LinearFunction\n",
    "    test_gradcheck = gradcheck(linear_custom, (x, weight, bias), eps=1e-6, atol=1e-4)\n",
    "    print('Gradient Check for Linear Module:', test_gradcheck)\n",
    "\n",
    "# Call the function\n",
    "gradient_check_and_output_check_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Consistency Check for ReLU Module: True\n",
      "Gradient Check for ReLU Module: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class ReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Compute ReLU activation\n",
    "        y = x.clamp(min=0)\n",
    "        \n",
    "        # Save input tensor for backward pass\n",
    "        ctx.save_for_backward(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout):\n",
    "        x, = ctx.saved_tensors\n",
    "        # Create a mask where x > 0\n",
    "        mask = (x > 0).double()\n",
    "        \n",
    "        # Compute gradient w.r.t. input x\n",
    "        dx = dout * mask\n",
    "        return dx\n",
    "\n",
    "def gradient_check_and_output_check_relu():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Create input data\n",
    "    x = torch.randn(5, 3, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "    # Custom ReLU function\n",
    "    relu_custom = ReLUFunction.apply\n",
    "    output_custom = relu_custom(x)\n",
    "\n",
    "    # PyTorch's built-in ReLU layer\n",
    "    relu_torch = torch.nn.ReLU().double()\n",
    "    output_torch = relu_torch(x)\n",
    "\n",
    "    # Compare outputs\n",
    "    print('Output Consistency Check for ReLU Module:', torch.allclose(output_custom, output_torch, atol=1e-6))\n",
    "\n",
    "    # Use gradcheck on the custom ReLUFunction\n",
    "    test_gradcheck = gradcheck(relu_custom, (x,), eps=1e-6, atol=1e-4)\n",
    "    print('Gradient Check for ReLU Module:', test_gradcheck)\n",
    "\n",
    "# Call the function\n",
    "gradient_check_and_output_check_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Consistency Check for Softmax Module: True\n",
      "Gradient Check for Softmax Module: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "class SoftMaxFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Subtract max for numerical stability\n",
    "        x_max = x.max(dim=1, keepdim=True)[0]\n",
    "        x_stable = x - x_max\n",
    "        \n",
    "        # Compute exponentials\n",
    "        exp_x = torch.exp(x_stable)\n",
    "        \n",
    "        # Compute sum over classes\n",
    "        sum_exp_x = exp_x.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute Softmax output\n",
    "        s = exp_x / sum_exp_x\n",
    "        \n",
    "        # Save the Softmax output for backward pass\n",
    "        ctx.save_for_backward(s)\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout):\n",
    "        s, = ctx.saved_tensors\n",
    "        \n",
    "        # Compute dot product of dout and s\n",
    "        s_dout = (dout * s).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute gradient w.r.t. input x\n",
    "        dx = s * (dout - s_dout)\n",
    "        return dx\n",
    "\n",
    "def gradient_check_and_output_check_softmax():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Create input data\n",
    "    x = torch.randn(5, 3, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "    # Custom Softmax function\n",
    "    softmax_custom = SoftMaxFunction.apply\n",
    "    output_custom = softmax_custom(x)\n",
    "\n",
    "    # PyTorch's built-in Softmax layer\n",
    "    softmax_torch = torch.nn.Softmax(dim=1).double()\n",
    "    output_torch = softmax_torch(x)\n",
    "\n",
    "    # Compare outputs\n",
    "    print('Output Consistency Check for Softmax Module:', torch.allclose(output_custom, output_torch, atol=1e-6))\n",
    "\n",
    "    # Use gradcheck on the custom SoftMaxFunction\n",
    "    test_gradcheck = gradcheck(softmax_custom, (x,), eps=1e-6, atol=1e-4)\n",
    "    print('Gradient Check for Softmax Module:', test_gradcheck)\n",
    "\n",
    "# Call the function\n",
    "gradient_check_and_output_check_softmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
