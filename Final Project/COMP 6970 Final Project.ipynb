{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cody Nichols' COMP 6970 Final Project\n",
    "This will be where I test and compile my final results from my findings.\n",
    "\n",
    "- Paper 1\n",
    "    - ~~Read Paper~~\n",
    "    - ~~Implement Bullish/Bearish Classification with NN Structure~~\n",
    "    - ~~Implement Candlestick Classification with NN Structure~~\n",
    "- Paper 2\n",
    "    - ~~Read Paper~~\n",
    "    - ~~Implement MS-CNN Architecture~~\n",
    "    - ~~Create DDQN Architecture~~\n",
    "- ~~Paper 3 (Time Permitting) (Decided Not To Do After Finishing Paper 2 Because of Complexity of Paper 2)~~\n",
    "- Complete Project\n",
    "    - Combine Code (in this ipynb)\n",
    "        - Implement Dataset Curation\n",
    "        - Create my own version of Paper 1\n",
    "        - Create my own version of Paper 2\n",
    "        - Combine the outputs of Paper 1 and 2's vision components as inputs for Paper 2's DDQN\n",
    "    - Write Paper\n",
    "    - Create Presentation\n",
    "    - Make Recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1 Rendition\n",
    "Paper 1 (Empowering Financial Technical Analysis Using Computer Vision Techniques) creates a CNN architecture that classifies candlestick charts in 2 ways. The first classification is a binary classification of bullish or bearish for a 20-candlestick image. The second classification is a multi-class classification of candlestick pattern type based on each individual candlestick and some surrounding candlesticks. The CNN architecture is 14-layer CNN with a 8 convolution layers, 3 max pooling layers, a VGG-16 pretrained preprocessing layer, a fully connected layer, and an output layer. This architecture was implemented in paper1.ipynb for the binary classification task, but was not used for the multi-class classification task. The reasoning behind not using it for the multi-class classification task was because of the ambiguity on how the model was trained. The assumed training method was dividing up the candlestick charts into smaller images to make classification on each candlestick and labeling each image easier, but there were some issues with the images I created not being big enough to make it all the way through without needing to size them back up. \n",
    "\n",
    "In this notebook, I will perform the same classification tasks with the knowledge that I have gained from implementing the author's solution to achieve the best accuracy on the binary classification and the best per-class accuracy for the multi-class classifcation. This per-class accuracy is because I will be having \"None\" labels in my candlesticks (unlike the author of Paper 1) because most candlesticks in candlestick charts do not have a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from pyts.image import GramianAngularField\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce GTX 1660 Ti\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device Checking\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataset(Dataset):\n",
    "    def __init__(self, file_path=None, daily_data=None, window_size=20, scaler=None):\n",
    "        \"\"\"\n",
    "        Initializes the AdvancedDataset.\n",
    "\n",
    "        Args:\n",
    "            file_path (str, optional): Path to the CSV file containing the data.\n",
    "            daily_data (list of pd.DataFrame, optional): Pre-split list of daily DataFrames.\n",
    "            window_size (int, optional): The size of the window for state representation. Defaults to 20.\n",
    "            scaler (StandardScaler, optional): A pre-fitted StandardScaler instance.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.scaler = scaler  # Assign the scaler\n",
    "\n",
    "        if daily_data is not None:\n",
    "            # Initialize with pre-split daily data\n",
    "            self.daily_data = daily_data\n",
    "            # If a scaler is provided, use it. Otherwise, raise an error\n",
    "            if self.scaler is None:\n",
    "                raise ValueError(\"Scaler must be provided when initializing with daily_data.\")\n",
    "        elif file_path is not None:\n",
    "            # Load and preprocess the data from the CSV file\n",
    "            self.data = pd.read_csv(file_path, parse_dates=[\"datetime\"])\n",
    "\n",
    "            # Ensure all required columns are present\n",
    "            required_columns = [\n",
    "                \"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                \"RSI_20\", \"Bollinger_High\", \"Bollinger_Low\", \"Bollinger_Middle\",\n",
    "                \"OBV\", \"ATR_20\", \"Stochastic_%K\", \"Stochastic_%D\",\n",
    "                \"VWAP\", \"SMA_7\", \"SMA_20\", \"EMA_7\", \"EMA_20\"\n",
    "            ]\n",
    "            missing_columns = set(required_columns) - set(self.data.columns)\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing columns in the data: {missing_columns}\")\n",
    "\n",
    "            # Extract the date component\n",
    "            self.data['date'] = self.data['datetime'].dt.date\n",
    "\n",
    "            # Handle missing values using ffill and bfill to avoid FutureWarning\n",
    "            self.data = self.data.ffill().bfill()\n",
    "\n",
    "            # Group the data by date\n",
    "            self.grouped = self.data.groupby('date')\n",
    "\n",
    "            # Split data into days, ensuring each day has at least `window_size` entries\n",
    "            self.daily_data = []\n",
    "            for date, group in self.grouped:\n",
    "                if len(group) >= self.window_size:\n",
    "                    self.daily_data.append(group.reset_index(drop=True))\n",
    "            \n",
    "            # Initialize the scaler if not provided\n",
    "            if self.scaler is None:\n",
    "                # Exclude 'datetime' and 'date' from scaling\n",
    "                feature_columns = [\n",
    "                    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                    \"RSI_20\", \"Bollinger_High\", \"Bollinger_Low\", \"Bollinger_Middle\",\n",
    "                    \"OBV\", \"ATR_20\", \"Stochastic_%K\", \"Stochastic_%D\",\n",
    "                    \"VWAP\", \"SMA_7\", \"SMA_20\", \"EMA_7\", \"EMA_20\"\n",
    "                ]\n",
    "                self.scaler = StandardScaler()\n",
    "                self.scaler.fit(self.data[feature_columns].values)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Either 'file_path' or 'daily_data' must be provided.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.daily_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the item at the given index or slice.\n",
    "\n",
    "        Args:\n",
    "            idx (int or slice): The index or slice to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict or AdvancedDataset: If idx is an int, returns a dictionary with processed data.\n",
    "                                      If idx is a slice, returns a new AdvancedDataset instance.\n",
    "        \"\"\"\n",
    "        if isinstance(idx, slice):\n",
    "            # Handle slicing by returning a new AdvancedDataset instance with sliced daily_data and the same scaler\n",
    "            sliced_data = self.daily_data[idx]\n",
    "            return AdvancedDataset(daily_data=sliced_data, window_size=self.window_size, scaler=self.scaler)\n",
    "        elif isinstance(idx, int):\n",
    "            # Handle single index access\n",
    "            day_data = self.daily_data[idx]\n",
    "\n",
    "            # Select all 18 numerical features (excluding 'datetime' and 'date')\n",
    "            feature_columns = [\n",
    "                \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "                \"RSI_20\", \"Bollinger_High\", \"Bollinger_Low\", \"Bollinger_Middle\",\n",
    "                \"OBV\", \"ATR_20\", \"Stochastic_%K\", \"Stochastic_%D\",\n",
    "                \"VWAP\", \"SMA_7\", \"SMA_20\", \"EMA_7\", \"EMA_20\"\n",
    "            ]\n",
    "            raw_data = day_data[feature_columns].values  # Shape: [num_entries, 18]\n",
    "\n",
    "            # Normalize the data\n",
    "            normalized_data = self._normalize(raw_data)  # Shape: [num_entries, 18]\n",
    "\n",
    "            states = []\n",
    "            for i in range(len(normalized_data) - self.window_size + 1):\n",
    "                state = normalized_data[i : i + self.window_size]  # Shape: [window_size, 18]\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).transpose(0, 1)  # Shape: [18, window_size]\n",
    "                state_tensor = state_tensor.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, 18, window_size]\n",
    "                states.append(state_tensor)\n",
    "            \n",
    "            return {\n",
    "                'states': states,  # List of tensors, each of shape [1, 1, 18, window_size]\n",
    "                'raw_data': raw_data,  # Raw numerical data\n",
    "                'datetimes': day_data['datetime'].values[self.window_size - 1 :],\n",
    "                'dates': day_data['date'].iloc[0]\n",
    "            }\n",
    "        else:\n",
    "            raise TypeError(\"Invalid index type. Must be int or slice.\")\n",
    "\n",
    "    def _normalize(self, data):\n",
    "        \"\"\"\n",
    "        Normalizes the data using standard scaling (zero mean, unit variance).\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): The raw OHLCV and technical indicators data. Shape: [num_entries, 18]\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The normalized data. Shape: [num_entries, 18]\n",
    "        \"\"\"\n",
    "        # Apply standard scaling (zero mean, unit variance)\n",
    "        return self.scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Scale 1D Convolution\n",
    "\n",
    "class SingleScale1D(nn.Module):\n",
    "    def __init__(self, input_channels=18, output_channels=5, kernel_size=3):\n",
    "        super(SingleScale1D, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 2D Convolution\n",
    "\n",
    "class ThreeByThreeConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=2, kernel_size=3):\n",
    "        super(ThreeByThreeConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5x5 2D Convolution\n",
    "\n",
    "class FiveByFiveConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, kernel_size=5):\n",
    "        super(FiveByFiveConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-scale Network\n",
    "\n",
    "class MultiScaleNet(nn.Module):\n",
    "    def __init__(self, height=18, width=20):\n",
    "        super(MultiScaleNet, self).__init__()\n",
    "        self.single_scale_1d = SingleScale1D(input_channels=18, output_channels=5, kernel_size=3)\n",
    "        self.three_by_three = ThreeByThreeConv2D(input_channels=1, output_channels=2, kernel_size=3)\n",
    "        self.five_by_five = FiveByFiveConv2D(input_channels=1, output_channels=1, kernel_size=5)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Assuming height=18 (features) and width=sequence_length\n",
    "        x_1d = x.view(batch_size, self.height, self.width)  # [batch, 18, width]\n",
    "        features_1d = self.single_scale_1d(x_1d)            # [batch, 5, width]\n",
    "\n",
    "        # Aggregate along the time dimension\n",
    "        features_1d = features_1d.mean(dim=2, keepdim=True)  # [batch, 5, 1]\n",
    "        features_1d = features_1d.view(batch_size, 5, 1, 1) # [batch, 5, 1, 1]\n",
    "        features_1d = F.interpolate(features_1d, size=(self.height, self.width), mode='bilinear', align_corners=False)  # [batch, 5, height, width]\n",
    "\n",
    "        features_3x3 = self.three_by_three(x)               # [batch, 2, height, width]\n",
    "        features_5x5 = self.five_by_five(x)                 # [batch, 1, height, width]\n",
    "\n",
    "        combined_features = torch.cat((features_1d, features_3x3, features_5x5), dim=1)  # [batch, 8, height, width]\n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Channel Attention (ECA) Block\n",
    "\n",
    "class ECABlock(nn.Module):\n",
    "    def __init__(self, channels, gamma=2, b=1):\n",
    "        super(ECABlock, self).__init__()\n",
    "        t = int(abs((math.log(channels, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)                             # [batch, channels, 1, 1]\n",
    "        y = y.squeeze(-1).transpose(-1, -2)             # [batch, 1, channels]\n",
    "        y = self.conv(y)                                 # [batch, 1, channels]\n",
    "        y = self.sigmoid(y)                              # [batch, 1, channels]\n",
    "        y = y.transpose(-1, -2).unsqueeze(-1)           # [batch, channels, 1]\n",
    "        return x * y.expand_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone Network\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, input_channels=8):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.eca = ECABlock(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)     # [batch, 64, H, W]\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)   # [batch, 64, H/2, W/2]\n",
    "        x = self.conv2(x)     # [batch, 64, H/2, W/2]\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.eca(x)       # [batch, 64, H/2, W/2]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Network with Backbone\n",
    "\n",
    "class MSNetWithBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSNetWithBackbone, self).__init__()\n",
    "        self.multi_scale_net = MultiScaleNet()\n",
    "        self.backbone = Backbone(input_channels=8)  # 5 + 2 + 1 from MultiScaleNet\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_scale_net(x)   # [batch, 8, H, W]\n",
    "        x = self.backbone(x)         # [batch, 64, H/2, W/2]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Network with Q-Value Output\n",
    "\n",
    "class MSNetWithQValue(nn.Module):\n",
    "    def __init__(self, num_actions=3):\n",
    "        super(MSNetWithQValue, self).__init__()\n",
    "        self.multi_scale_with_backbone = MSNetWithBackbone()\n",
    "\n",
    "        self.conv_final = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Determine the size after convolution\n",
    "        sample_input = torch.randn(1, 1, 18, 20)  # [batch, channels, height=18, width=20]\n",
    "        sample_output = self.multi_scale_with_backbone(sample_input)\n",
    "        sample_output = self.conv_final(sample_output)\n",
    "        sample_output = F.relu(sample_output)\n",
    "        feature_map_size = sample_output.view(-1).size(0)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_map_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_scale_with_backbone(x)  # [batch, 64, H/2, W/2]\n",
    "        x = self.conv_final(x)                 # [batch, 64, H/2, W/2]\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)                    # [batch, 64 * (H/2) * (W/2)]\n",
    "        x = self.relu(self.fc1(x))            # [batch, 128]\n",
    "        x = self.fc2(x)                        # [batch, num_actions]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create MS-CNN and ensure the output shape is the correct size (3 Q values: 1 for each action)\n",
    "\n",
    "model = MSNetWithQValue(num_actions=3)\n",
    "test_input = torch.randn(1, 1, 18, 20)\n",
    "output = model(test_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    # Initialize buffer as a double ended queue\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # Push a state, action, reward, next_state, done tuple on the buffer\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Randomly sample\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        state_batch = torch.cat(state, dim=0)\n",
    "        next_state_batch = torch.cat(next_state, dim=0)\n",
    "        return (\n",
    "            state_batch,\n",
    "            torch.tensor(action, dtype=torch.long),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            next_state_batch,\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    # Override buffer's length method and return length\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, buffer_capacity, gamma=0.99, lr=1e-3, target_update_freq=100):\n",
    "        self.actor = MSNetWithQValue()\n",
    "        self.target = MSNetWithQValue()\n",
    "        self.target.load_state_dict(self.actor.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.position = 'sold_out'\n",
    "\n",
    "    # Select an action based on current state\n",
    "    def select_action(self, state):\n",
    "        # Exploration block. \n",
    "        # If bought in and told to hold or buy randomly, hold. \n",
    "        # If sold out and told to hold or sell randomly, sell.\n",
    "        if random.random() < self.epsilon:\n",
    "            random_action = random.randint(0,2)\n",
    "            if self.position == 'bought_in':\n",
    "                if random_action in [0, 1]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 2\n",
    "            else:\n",
    "                if random_action in [0, 2]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "        # Eploitation block.\n",
    "        # Select action with highest Q value\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.actor(state)\n",
    "                max_q, max_action = q_values.max(dim=1)\n",
    "                max_action = max_action.item()\n",
    "                \n",
    "                if self.position == 'bought_in':\n",
    "                    if max_action in [0, 1]:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 2\n",
    "                else:\n",
    "                    if max_action in [0, 2]:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.actor(next_state_batch)\n",
    "            next_actions = next_q_values.argmax(dim=1)\n",
    "            target_q_values = self.target(next_state_batch)\n",
    "            target_q = reward_batch + self.gamma * (1 - done_batch) * target_q_values[range(batch_size), next_actions]\n",
    "\n",
    "        current_q = self.actor(state_batch)[range(batch_size), action_batch]\n",
    "\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def eval_mode(self):\n",
    "        self.actor.eval()\n",
    "        self.target.eval()\n",
    "        self.epsilon = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnvWithFeatures:\n",
    "    def __init__(self, daily_dataset, k=5, testing=False):\n",
    "        self.daily_dataset = daily_dataset\n",
    "        self.current_day = 0\n",
    "        self.num_days = len(daily_dataset)\n",
    "        self.k = k\n",
    "        self.testing = testing\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.price_history = []\n",
    "        self.initial_cash = 500000\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.trade_log = []\n",
    "        self.cost_basis = 0.0\n",
    "        self.shares_held = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.testing = False\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.price_history = []\n",
    "        self.initial_cash = 500000\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.trade_log = []\n",
    "        self.cost_basis = 0.0\n",
    "        self.shares_held = 0\n",
    "\n",
    "        if self.current_day >= self.num_days:\n",
    "            self.done = True\n",
    "            return None\n",
    "\n",
    "        day_data = self.daily_dataset[self.current_day]\n",
    "        self.states = day_data['states']\n",
    "        self.raw_data = day_data['raw_data']\n",
    "        self.datetimes = day_data['datetimes']\n",
    "        self.window_size = self.states[0].shape[-1]\n",
    "        self.num_steps = len(self.states)\n",
    "        self.raw_close_prices = self.raw_data[self.window_size - 1 :, 3]\n",
    "\n",
    "        self.current_day += 1\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        if self.current_step >= self.num_steps:\n",
    "            self.done = True\n",
    "            return None\n",
    "\n",
    "        state = self.states[self.current_step]\n",
    "        current_close_price = self.raw_data[self.current_step + self.window_size - 1, 3]\n",
    "        self.price_history.append(current_close_price)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step + 1 < self.num_steps:\n",
    "            state = self.states[self.current_step]\n",
    "            current_close_price = self.raw_close_prices[self.current_step]\n",
    "            next_close_price = self.raw_close_prices[self.current_step + 1]\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            profit = 0\n",
    "\n",
    "            if action == 1:\n",
    "                if current_close_price == 0:\n",
    "                    num_shares = 0\n",
    "                else:\n",
    "                    num_shares = int(self.cash_balance / current_close_price)\n",
    "                total_cost = num_shares * current_close_price\n",
    "                self.cash_balance -= total_cost\n",
    "                self.shares_held += num_shares\n",
    "                self.cost_basis += total_cost\n",
    "\n",
    "            elif action == 2:\n",
    "                num_shares = self.shares_held\n",
    "                total_proceeds = num_shares * current_close_price\n",
    "                self.cash_balance += total_proceeds\n",
    "                self.shares_held = 0\n",
    "                profit = total_proceeds - self.cost_basis\n",
    "                self.cost_basis = 0\n",
    "            \n",
    "            else:\n",
    "                profit = 0.0\n",
    "\n",
    "            self.previous_total_asset_value = self.total_asset_value\n",
    "            self.total_asset_value = self.cash_balance + self.shares_held * current_close_price\n",
    "\n",
    "            profit = self.total_asset_value - self.previous_total_asset_value\n",
    "\n",
    "            if self.testing:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.calculate_reward(action, current_close_price)\n",
    "\n",
    "            action_mapping = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
    "            trade_info = {\n",
    "                'datetime': datetime,\n",
    "                'action': action_mapping[action],\n",
    "                'reward': reward,\n",
    "                'profit': profit,\n",
    "                'total_asset_value': self.total_asset_value,\n",
    "                'cash_balance': self.cash_balance,\n",
    "                'shares_held': self.shares_held,\n",
    "                'position': 'bought_in' if self.shares_held > 0 else 'sold_out'\n",
    "            }\n",
    "            self.trade_log.append(trade_info)\n",
    "\n",
    "            self.current_step += 1\n",
    "            done = self.current_step >= self.num_steps - 1\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            self.done = True\n",
    "            return None, 0, True, {}\n",
    "\n",
    "    def calculate_reward(self, action, current_close_price):\n",
    "        POS_t = 1 if self.shares_held > 0 else 0\n",
    "\n",
    "        R_k_t = []\n",
    "        for i in range(1, self.k + 1):\n",
    "            future_step = self.current_step + i\n",
    "            if future_step >= self.num_steps:\n",
    "                break\n",
    "            future_price = self.raw_close_prices[future_step]\n",
    "            if current_close_price == 0:\n",
    "                R_k_t_i = 0\n",
    "            else:\n",
    "                R_k_t_i = (future_price - current_close_price) / current_close_price\n",
    "            R_k_t.append(R_k_t_i)\n",
    "\n",
    "        if len(R_k_t) < 2:\n",
    "            SR_t = 0\n",
    "        else:\n",
    "            mean_R = np.mean(R_k_t)\n",
    "            std_R = np.std(R_k_t)\n",
    "            if std_R == 0:\n",
    "                SR_t = 0\n",
    "            else:\n",
    "                SR_t = mean_R / std_R\n",
    "\n",
    "        SSR_t = POS_t * SR_t\n",
    "\n",
    "        return SSR_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Days: 369\n",
      "Testing Days: 93\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./labeling/TQQQ_minute_data_cleaned_advanced.csv\"\n",
    "dataset = AdvancedDataset(file_path)\n",
    "num_days = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * num_days)\n",
    "test_size = num_days - train_size\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "print(f\"Training Days: {len(train_dataset)}\")\n",
    "print(f\"Testing Days: {len(test_dataset)}\")\n",
    "\n",
    "env_train = StockTradingEnvWithFeatures(train_dataset)\n",
    "env_test = StockTradingEnvWithFeatures(test_dataset, testing=True)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DDQNAgent(buffer_capacity=10000)\n",
    "batch_size = 32\n",
    "\n",
    "train_log_dir = \"./training_logs/\"\n",
    "test_log_dir = \"./testing_logs/\"\n",
    "os.makedirs(train_log_dir, exist_ok=True)\n",
    "os.makedirs(test_log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Day 1/369, Total Reward: 43.94, Total Profit: -13129.23\n",
      "Training Day 2/369, Total Reward: -67.87, Total Profit: 1831.91\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Loop\n",
    "'''\n",
    "for day in range(len(train_dataset)):\n",
    "    state = env_train.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done and state is not None:\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env_train.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        agent.train(batch_size)\n",
    "\n",
    "        if action == 1:\n",
    "            agent.position = 'bought_in'\n",
    "        elif action == 2:\n",
    "            agent.position = 'sold_out'\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if env_train.shares_held > 0:\n",
    "        last_close_price = env_train.raw_data[env_train.current_step + env_train.window_size - 1, 3]\n",
    "        total_proceeds = env_train.shares_held * last_close_price\n",
    "        profit = total_proceeds - env_train.cost_basis\n",
    "        env_train.cash_balance += total_proceeds\n",
    "        env_train.shares_held = 0\n",
    "        env_train.cost_basis = 0.0\n",
    "        env_train.total_asset_value = env_train.cash_balance\n",
    "\n",
    "        trade_info = {\n",
    "            'datetime': env_train.datetimes[-1],\n",
    "            'action': 'Sell (EOD)',\n",
    "            'reward': 0.0,\n",
    "            'profit': profit,\n",
    "            'total_asset_value': env_train.total_asset_value,\n",
    "            'cash_balance': env_train.cash_balance,\n",
    "            'shares_held': env_train.shares_held,\n",
    "            'position': 'sold_out'\n",
    "        }\n",
    "        env_train.trade_log.append(trade_info)\n",
    "\n",
    "        agent.position = 'sold_out'\n",
    "\n",
    "    total_profit = env_train.total_asset_value - env_train.initial_cash\n",
    "    print(f\"Training Day {day + 1}/{len(train_dataset)}, Total Reward: {total_reward:.2f}, Total Profit: {total_profit:.2f}\")\n",
    "\n",
    "    trade_log = env_train.trade_log\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"{train_log_dir}/2trade_log_day_{day + 1}.csv\", index=False)\n",
    "\n",
    "    env_train.trade_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing Loop\n",
    "'''\n",
    "torch.save(agent.actor.state_dict(), 'ddqn_actor3.pth')\n",
    "torch.save(agent.target.state_dict(), 'ddqn_target3.pth')\n",
    "print(\"Agent's actor and target networks have been saved successfully.\")\n",
    "\n",
    "loaded_agent = DDQNAgent(buffer_capacity=10000)\n",
    "loaded_agent.actor.load_state_dict(torch.load('ddqn_actor3.pth', weights_only=True))\n",
    "loaded_agent.target.load_state_dict(torch.load('ddqn_target3.pth', weights_only=True))\n",
    "loaded_agent.eval_mode()\n",
    "\n",
    "# print(\"Agent's actor and target networks have been loaded and set to evaluation mode successfully.\")\n",
    "\n",
    "for day in range(len(test_dataset)):\n",
    "    state = env_test.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done and state is not None:\n",
    "        action = loaded_agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env_test.step(action)\n",
    "\n",
    "        if action == 1:\n",
    "            agent.position = 'bought_in'\n",
    "        elif action == 2:\n",
    "            agent.position = 'sold_out'\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if env_test.shares_held > 0:\n",
    "        last_close_price = env_test.raw_data[env_test.current_step + env_test.window_size - 1, 3]\n",
    "        total_proceeds = env_test.shares_held * last_close_price\n",
    "        profit = total_proceeds - env_test.cost_basis\n",
    "        env_test.cash_balance += total_proceeds\n",
    "        env_test.shares_held = 0\n",
    "        env_test.cost_basis = 0.0\n",
    "        env_test.total_asset_value = env_test.cash_balance\n",
    "\n",
    "        trade_info = {\n",
    "            'datetime': env_test.datetimes[-1],\n",
    "            'action': 'Sell (EOD)',\n",
    "            'reward': 0.0,\n",
    "            'profit': profit,\n",
    "            'total_asset_value': env_test.total_asset_value,\n",
    "            'cash_balance': env_test.cash_balance,\n",
    "            'shares_held': env_test.shares_held,\n",
    "            'position': 'sold_out'\n",
    "        }\n",
    "        env_test.trade_log.append(trade_info)\n",
    "\n",
    "        agent.position = 'sold_out'\n",
    "\n",
    "    total_profit = env_test.total_asset_value - env_test.initial_cash\n",
    "    print(f\"Testing Day {day + 1}/{len(test_dataset)}, Total Reward: {total_reward:.2f}, Total Profit: {total_profit:.2f}\")\n",
    "\n",
    "    trade_log = env_test.trade_log\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"{test_log_dir}/2trade_log_day_{day + 1}.csv\", index=False)\n",
    "\n",
    "    env_test.trade_log = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
