{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2 Implementation\n",
    "\n",
    "This is the ipynb where I implemented the architecture that Paper 2 discusses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce GTX 1660 Ti\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the class for the open, high, low, close, volume dataset to be used by the MS-CNN and DDQN Agent.\n",
    "'''\n",
    "\n",
    "class OHLCVDataset(Dataset):\n",
    "    def __init__(self, file_path=None, daily_data=None, window_size=20):\n",
    "        '''\n",
    "        Initialize the dataset to have 20 minute windows\n",
    "        '''\n",
    "        self.window_size = window_size\n",
    "\n",
    "        if daily_data is not None:\n",
    "            self.daily_data = daily_data\n",
    "        elif file_path is not None:\n",
    "            self.data = pd.read_csv(file_path, parse_dates=[\"datetime\"])\n",
    "            self.raw_data = self.data[[\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "            self.data['date'] = self.data['datetime'].dt.date\n",
    "\n",
    "            self.grouped = self.data.groupby('date')\n",
    "\n",
    "            self.daily_data = []\n",
    "            for date, group in self.grouped:\n",
    "                if len(group) >= self.window_size:\n",
    "                    self.daily_data.append(group.reset_index(drop=True))\n",
    "        else:\n",
    "            raise ValueError(\"Either 'file_path' or 'daily_data' must be provided.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.daily_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            sliced_data = self.daily_data[idx]\n",
    "            return OHLCVDataset(daily_data=sliced_data, window_size=self.window_size)\n",
    "        elif isinstance(idx, int):\n",
    "            day_data = self.daily_data[idx]\n",
    "            raw_data = day_data[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values\n",
    "            normalized_data = self._normalize(raw_data)\n",
    "\n",
    "            states = []\n",
    "            for i in range(len(normalized_data) - self.window_size + 1):\n",
    "                state = normalized_data[i : i + self.window_size]\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).transpose(0, 1)\n",
    "                state_tensor = state_tensor.unsqueeze(0).unsqueeze(0)\n",
    "                states.append(state_tensor)\n",
    "            \n",
    "            return {\n",
    "                'states': states,\n",
    "                'raw_data': raw_data,\n",
    "                'datetimes': day_data['datetime'].values[self.window_size - 1 :],\n",
    "                'dates': day_data['date'].iloc[0]\n",
    "            }\n",
    "        else:\n",
    "            raise TypeError(\"Invalid index type. Must be int or slice.\")\n",
    "\n",
    "    def _normalize(self, data):\n",
    "        min_vals = data.min(axis=0)\n",
    "        max_vals = data.max(axis=0)\n",
    "        return (data - min_vals) / (max_vals - min_vals + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single Scale Network According to Paper's Description\n",
    "'''\n",
    "\n",
    "class SingleScale1D(nn.Module):\n",
    "    def __init__(self, input_channels=5, output_channels=5, kernel_size=3):\n",
    "        super(SingleScale1D, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3x3 Network According to Paper's Description\n",
    "'''\n",
    "\n",
    "class ThreeByThreeConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=2, kernel_size=3):\n",
    "        super(ThreeByThreeConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5x5 Network According to Paper's Description\n",
    "'''\n",
    "\n",
    "class FiveByFiveConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, kernel_size=5):\n",
    "        super(FiveByFiveConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combination of Networks According to Paper's Description\n",
    "'''\n",
    "\n",
    "class MultiScaleNet(nn.Module):\n",
    "    def __init__(self, height=5, width=20):\n",
    "        super(MultiScaleNet, self).__init__()\n",
    "        self.single_scale_1d = SingleScale1D(input_channels=5, output_channels=5, kernel_size=3)\n",
    "        self.three_by_three = ThreeByThreeConv2D(input_channels=1, output_channels=2, kernel_size=3)\n",
    "        self.five_by_five = FiveByFiveConv2D(input_channels=1, output_channels=1, kernel_size=5)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        x_1d = x.view(batch_size, height, width)\n",
    "        features_1d = self.single_scale_1d(x_1d)\n",
    "\n",
    "        features_1d = features_1d.mean(dim=2, keepdim=True)\n",
    "        features_1d = features_1d.view(batch_size, 5, 1, 1)\n",
    "        features_1d = F.interpolate(features_1d, size=(height, width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        features_3x3 = self.three_by_three(x)\n",
    "        features_5x5 = self.five_by_five(x)\n",
    "\n",
    "        combined_features = torch.cat((features_1d, features_3x3, features_5x5), dim=1)\n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attention Block According to Paper's Description\n",
    "'''\n",
    "\n",
    "class ECABlock(nn.Module):\n",
    "    def __init__(self, channels, gamma=2, b=1):\n",
    "        super(ECABlock, self).__init__()\n",
    "        t = int(abs((math.log(channels, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = y.squeeze(-1).transpose(-1, -2)\n",
    "        y = self.conv(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = y.transpose(-1, -2).unsqueeze(-1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Backbone Network According to Paper's Description\n",
    "'''\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, input_channels=8):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.eca = ECABlock(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.eca(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combination of MS-Net and Backbone Network According to Paper's Description\n",
    "'''\n",
    "\n",
    "class MSNetWithBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSNetWithBackbone, self).__init__()\n",
    "        self.multi_scale_net = MultiScaleNet()\n",
    "        self.backbone = Backbone(input_channels=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_scale_net(x)\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full Implementation of MS-CNN with Q-value output According to Paper's Description\n",
    "'''\n",
    "\n",
    "class MSNetWithQValue(nn.Module):\n",
    "    def __init__(self, num_actions=3):\n",
    "        super(MSNetWithQValue, self).__init__()\n",
    "        self.multi_scale_with_backbone = MSNetWithBackbone()\n",
    "\n",
    "        self.conv_final = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        sample_input = torch.randn(1, 1, 5, 20)\n",
    "        sample_output = self.multi_scale_with_backbone(sample_input)\n",
    "        sample_output = self.conv_final(sample_output)\n",
    "        feature_map_size = sample_output.view(-1).size(0)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_map_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_scale_with_backbone(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create MS-CNN and ensure the output shape is the correct size (3 Q values: 1 for each action)\n",
    "model = MSNetWithQValue(num_actions=3)\n",
    "test_input = torch.randn(1, 1, 5, 20)\n",
    "output = model(test_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replay Buffer for DDQNAgent\n",
    "'''\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # Initialize buffer as a double ended queue\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # Push a state, action, reward, next_state, done tuple on the buffer\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Randomly sample\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        state_batch = torch.cat(state, dim=0)\n",
    "        next_state_batch = torch.cat(next_state, dim=0)\n",
    "        return (\n",
    "            state_batch,\n",
    "            torch.tensor(action, dtype=torch.long),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            next_state_batch,\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    # Override buffer's length method and return length\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Double Deep Q-Network Agent:\n",
    "This Agent has an actor network and a target network where the actor selects the action\n",
    "and the target evaluates it to decouple the two tasks and prevent overestimate of Q-values.\n",
    "'''\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, buffer_capacity, gamma=0.99, lr=1e-3, target_update_freq=100):\n",
    "        '''\n",
    "        Initialize Agent to have 2 networks, a buffer, and necessary variables.\n",
    "        '''\n",
    "        self.actor = MSNetWithQValue()\n",
    "        self.target = MSNetWithQValue()\n",
    "        self.target.load_state_dict(self.actor.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.position = 'sold_out'\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Select an action randomly or based on highest Q-value.\n",
    "        '''\n",
    "        # Exploration block. \n",
    "        # If bought in and told to hold or buy randomly, hold. \n",
    "        # If sold out and told to hold or sell randomly, sell.\n",
    "        if random.random() < self.epsilon:\n",
    "            random_action = random.randint(0,2)\n",
    "            if self.position == 'bought_in':\n",
    "                if random_action in [0, 1]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 2\n",
    "            else:\n",
    "                if random_action in [0, 2]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "        # Eploitation block.\n",
    "        # Select action with highest Q value\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.actor(state)\n",
    "                max_q, max_action = q_values.max(dim=1)\n",
    "                max_action = max_action.item()\n",
    "                \n",
    "                if self.position == 'bought_in':\n",
    "                    if max_action in [0, 1]:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 2\n",
    "                else:\n",
    "                    if max_action in [0, 2]:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        '''\n",
    "        Train the agent by updating its weights based on the decision it most recently made.\n",
    "        '''\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Get batch from replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.buffer.sample(batch_size)\n",
    "\n",
    "        # Calculate Q-values for actor and target networks\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.actor(next_state_batch)\n",
    "            next_actions = next_q_values.argmax(dim=1)\n",
    "            target_q_values = self.target(next_state_batch)\n",
    "            target_q = reward_batch + self.gamma * (1 - done_batch) * target_q_values[range(batch_size), next_actions]\n",
    "\n",
    "        # Get Q-values in current state\n",
    "        current_q = self.actor(state_batch)[range(batch_size), action_batch]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "        # Update weights in network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network to have the same weights as actor.\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Increment steps and change epsilon value\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Add transition to replay buffer.\n",
    "        '''\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def eval_mode(self):\n",
    "        '''\n",
    "        Set Agent to eval mode, so no random actions can be taken anymore.\n",
    "        '''\n",
    "        self.actor.eval()\n",
    "        self.target.eval()\n",
    "        self.epsilon = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trading Environment that the Agent will be on.\n",
    "'''\n",
    "\n",
    "class StockTradingEnvWithFeatures:\n",
    "    def __init__(self, daily_dataset, k=5, testing=False):\n",
    "        '''\n",
    "        All relevant variables for the environment.\n",
    "        '''\n",
    "        self.daily_dataset = daily_dataset\n",
    "        self.current_day = 0\n",
    "        self.num_days = len(daily_dataset)\n",
    "        self.k = k\n",
    "        self.testing = testing\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.price_history = []\n",
    "        self.initial_cash = 500000\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.trade_log = []\n",
    "        self.cost_basis = 0.0\n",
    "        self.shares_held = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment when a new environment is created (in this case every day the environment is reset).\n",
    "        '''\n",
    "        self.testing = False\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.price_history = []\n",
    "        self.initial_cash = 500000\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.trade_log = []\n",
    "        self.cost_basis = 0.0\n",
    "        self.shares_held = 0\n",
    "\n",
    "        if self.current_day >= self.num_days:\n",
    "            self.done = True\n",
    "            return None\n",
    "\n",
    "        day_data = self.daily_dataset[self.current_day]\n",
    "        self.states = day_data['states']\n",
    "        self.raw_data = day_data['raw_data']\n",
    "        self.datetimes = day_data['datetimes']\n",
    "        self.window_size = self.states[0].shape[-1]\n",
    "        self.num_steps = len(self.states)\n",
    "        self.raw_close_prices = self.raw_data[self.window_size - 1 :, 3]\n",
    "\n",
    "        self.current_day += 1\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Gets current state of the environment.\n",
    "        '''\n",
    "        if self.current_step >= self.num_steps:\n",
    "            self.done = True\n",
    "            return None\n",
    "\n",
    "        state = self.states[self.current_step]\n",
    "        current_close_price = self.raw_data[self.current_step + self.window_size - 1, 3]\n",
    "        self.price_history.append(current_close_price)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Simulates taking a step/action in the environment and returns what happened as a result.\n",
    "        '''\n",
    "        if self.current_step + 1 < self.num_steps:\n",
    "            state = self.states[self.current_step]\n",
    "            current_close_price = self.raw_close_prices[self.current_step]\n",
    "            next_close_price = self.raw_close_prices[self.current_step + 1]\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            profit = 0\n",
    "\n",
    "            if action == 1:\n",
    "                if current_close_price == 0:\n",
    "                    num_shares = 0\n",
    "                else:\n",
    "                    num_shares = int(self.cash_balance / current_close_price)\n",
    "                total_cost = num_shares * current_close_price\n",
    "                self.cash_balance -= total_cost\n",
    "                self.shares_held += num_shares\n",
    "                self.cost_basis += total_cost\n",
    "\n",
    "            elif action == 2:\n",
    "                num_shares = self.shares_held\n",
    "                total_proceeds = num_shares * current_close_price\n",
    "                self.cash_balance += total_proceeds\n",
    "                self.shares_held = 0\n",
    "                profit = total_proceeds - self.cost_basis\n",
    "                self.cost_basis = 0\n",
    "            \n",
    "            else:\n",
    "                profit = 0.0\n",
    "\n",
    "            self.previous_total_asset_value = self.total_asset_value\n",
    "            self.total_asset_value = self.cash_balance + self.shares_held * current_close_price\n",
    "\n",
    "            profit = self.total_asset_value - self.previous_total_asset_value\n",
    "\n",
    "            if self.testing:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.calculate_reward(action, current_close_price)\n",
    "\n",
    "            action_mapping = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
    "            trade_info = {\n",
    "                'datetime': datetime,\n",
    "                'action': action_mapping[action],\n",
    "                'reward': reward,\n",
    "                'profit': profit,\n",
    "                'total_asset_value': self.total_asset_value,\n",
    "                'cash_balance': self.cash_balance,\n",
    "                'shares_held': self.shares_held,\n",
    "                'position': 'bought_in' if self.shares_held > 0 else 'sold_out'\n",
    "            }\n",
    "            self.trade_log.append(trade_info)\n",
    "\n",
    "            self.current_step += 1\n",
    "            done = self.current_step >= self.num_steps - 1\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            self.done = True\n",
    "            return None, 0, True, {}\n",
    "\n",
    "    def calculate_reward(self, action, current_close_price):\n",
    "        '''\n",
    "        Calculates the reward based on the paper's specifications.\n",
    "        '''\n",
    "        POS_t = 1 if self.shares_held > 0 else 0\n",
    "\n",
    "        R_k_t = []\n",
    "        for i in range(1, self.k + 1):\n",
    "            future_step = self.current_step + i\n",
    "            if future_step >= self.num_steps:\n",
    "                break\n",
    "            future_price = self.raw_close_prices[future_step]\n",
    "            if current_close_price == 0:\n",
    "                R_k_t_i = 0\n",
    "            else:\n",
    "                R_k_t_i = (future_price - current_close_price) / current_close_price\n",
    "            R_k_t.append(R_k_t_i)\n",
    "\n",
    "        if len(R_k_t) < 2:\n",
    "            SR_t = 0\n",
    "        else:\n",
    "            mean_R = np.mean(R_k_t)\n",
    "            std_R = np.std(R_k_t)\n",
    "            if std_R == 0:\n",
    "                SR_t = 0\n",
    "            else:\n",
    "                SR_t = mean_R / std_R\n",
    "\n",
    "        SSR_t = POS_t * SR_t\n",
    "\n",
    "        return SSR_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Days: 369\n",
      "Testing Days: 93\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Split the dataset 80/20 into training and testing respectively.\n",
    "'''\n",
    "\n",
    "file_path = \"./labeling/SPXL_minute_data_cleaned.csv\"\n",
    "dataset = OHLCVDataset(file_path)\n",
    "num_days = len(dataset)\n",
    "\n",
    "train_size = int(0.8 * num_days)\n",
    "test_size = num_days - train_size\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "print(f\"Training Days: {len(train_dataset)}\")\n",
    "print(f\"Testing Days: {len(test_dataset)}\")\n",
    "\n",
    "env_train = StockTradingEnvWithFeatures(train_dataset)\n",
    "env_test = StockTradingEnvWithFeatures(test_dataset, testing=True)\n",
    "\n",
    "agent = DDQNAgent(buffer_capacity=10000)\n",
    "batch_size = 32\n",
    "\n",
    "train_log_dir = \"./training_logs/\"\n",
    "test_log_dir = \"./testing_logs/\"\n",
    "os.makedirs(train_log_dir, exist_ok=True)\n",
    "os.makedirs(test_log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform Training\n",
    "'''\n",
    "\n",
    "for day in range(len(train_dataset)):\n",
    "    state = env_train.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # While not at EOD or a random unforeseen error occurs, keep training\n",
    "    while not done and state is not None:\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env_train.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        agent.train(batch_size)\n",
    "\n",
    "        if action == 1:\n",
    "            agent.position = 'bought_in'\n",
    "        elif action == 2:\n",
    "            agent.position = 'sold_out'\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Sell at EOD and log\n",
    "    if env_train.shares_held > 0:\n",
    "        last_close_price = env_train.raw_data[env_train.current_step + env_train.window_size - 1, 3]\n",
    "        total_proceeds = env_train.shares_held * last_close_price\n",
    "        profit = total_proceeds - env_train.cost_basis\n",
    "        env_train.cash_balance += total_proceeds\n",
    "        env_train.shares_held = 0\n",
    "        env_train.cost_basis = 0.0\n",
    "        env_train.total_asset_value = env_train.cash_balance\n",
    "\n",
    "        trade_info = {\n",
    "            'datetime': env_train.datetimes[-1],\n",
    "            'action': 'Sell (EOD)',\n",
    "            'reward': 0.0,\n",
    "            'profit': profit,\n",
    "            'total_asset_value': env_train.total_asset_value,\n",
    "            'cash_balance': env_train.cash_balance,\n",
    "            'shares_held': env_train.shares_held,\n",
    "            'position': 'sold_out'\n",
    "        }\n",
    "        env_train.trade_log.append(trade_info)\n",
    "\n",
    "        agent.position = 'sold_out'\n",
    "\n",
    "    # Calculate Profit for Output\n",
    "    total_profit = env_train.total_asset_value - env_train.initial_cash\n",
    "    print(f\"Training Day {day + 1}/{len(train_dataset)}, Total Reward: {total_reward:.2f}, Total Profit: {total_profit:.2f}\")\n",
    "\n",
    "    # Log\n",
    "    trade_log = env_train.trade_log\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"{train_log_dir}/2trade_log_day_{day + 1}.csv\", index=False)\n",
    "\n",
    "    # Empty for next loop\n",
    "    env_train.trade_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's actor and target networks have been loaded and set to evaluation mode successfully.\n",
      "Testing Day 1/93, Total Reward: 59.05, Total Profit: 3154.68\n",
      "Testing Day 2/93, Total Reward: 71.29, Total Profit: 4352.29\n",
      "Testing Day 3/93, Total Reward: 76.25, Total Profit: -337.60\n",
      "Testing Day 4/93, Total Reward: -16.33, Total Profit: -6971.76\n",
      "Testing Day 5/93, Total Reward: 9.83, Total Profit: -747.34\n",
      "Testing Day 6/93, Total Reward: 212.23, Total Profit: 10133.64\n",
      "Testing Day 7/93, Total Reward: 83.26, Total Profit: 5498.13\n",
      "Testing Day 8/93, Total Reward: 182.79, Total Profit: 6913.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# While not at EOD or a random unforeseen error occurs, keep training\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env_test\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mDDQNAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     51\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m---> 52\u001b[0m     max_q, max_action \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     max_action \u001b[38;5;241m=\u001b[39m max_action\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbought_in\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing Loop\n",
    "'''\n",
    "\n",
    "# Save Network weights for future use (training loops take an hour, so I needed this)\n",
    "# torch.save(agent.actor.state_dict(), './Model Weights/ddqn_actor2.pth')\n",
    "# torch.save(agent.target.state_dict(), './Model Weights/ddqn_target2.pth')\n",
    "# print(\"Agent's actor and target networks have been saved successfully.\")\n",
    "\n",
    "# Load Networks to perform testing and set agents to evaluation mode.\n",
    "loaded_agent = DDQNAgent(buffer_capacity=10000)\n",
    "loaded_agent.actor.load_state_dict(torch.load('./Model Weights/ddqn_actor2.pth', weights_only=True))\n",
    "loaded_agent.target.load_state_dict(torch.load('./Model Weights/ddqn_target2.pth', weights_only=True))\n",
    "loaded_agent.eval_mode()\n",
    "\n",
    "print(loaded_agent.actor.state_dict())\n",
    "\n",
    "print(\"Agent's actor and target networks have been loaded and set to evaluation mode successfully.\")\n",
    "\n",
    "for day in range(len(test_dataset)):\n",
    "    state = env_test.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # While not at EOD or a random unforeseen error occurs, keep training\n",
    "    while not done and state is not None:\n",
    "        action = loaded_agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env_test.step(action)\n",
    "\n",
    "        if action == 1:\n",
    "            agent.position = 'bought_in'\n",
    "        elif action == 2:\n",
    "            agent.position = 'sold_out'\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Sell at EOD and log\n",
    "    if env_test.shares_held > 0:\n",
    "        last_close_price = env_test.raw_data[env_test.current_step + env_test.window_size - 1, 3]\n",
    "        total_proceeds = env_test.shares_held * last_close_price\n",
    "        profit = total_proceeds - env_test.cost_basis\n",
    "        env_test.cash_balance += total_proceeds\n",
    "        env_test.shares_held = 0\n",
    "        env_test.cost_basis = 0.0\n",
    "        env_test.total_asset_value = env_test.cash_balance\n",
    "\n",
    "        trade_info = {\n",
    "            'datetime': env_test.datetimes[-1],\n",
    "            'action': 'Sell (EOD)',\n",
    "            'reward': 0.0,\n",
    "            'profit': profit,\n",
    "            'total_asset_value': env_test.total_asset_value,\n",
    "            'cash_balance': env_test.cash_balance,\n",
    "            'shares_held': env_test.shares_held,\n",
    "            'position': 'sold_out'\n",
    "        }\n",
    "        env_test.trade_log.append(trade_info)\n",
    "\n",
    "        agent.position = 'sold_out'\n",
    "\n",
    "    # Calculate Profit for Output\n",
    "    total_profit = env_test.total_asset_value - env_test.initial_cash\n",
    "    print(f\"Testing Day {day + 1}/{len(test_dataset)}, Total Reward: {total_reward:.2f}, Total Profit: {total_profit:.2f}\")\n",
    "\n",
    "    # Log\n",
    "    trade_log = env_test.trade_log\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"{test_log_dir}/2trade_log_day_{day + 1}.csv\", index=False)\n",
    "\n",
    "    # Empty for next loop\n",
    "    env_test.trade_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This was my initial train loop before I created different data and just wanted to make sure the training was working.\n",
    "'''\n",
    "\n",
    "file_path = \"./labeling/TQQQ_minute_data_cleaned_labeled_375days.csv\"\n",
    "dataset = OHLCVDataset(file_path)\n",
    "env = StockTradingEnvWithFeatures(dataset)\n",
    "agent = DDQNAgent(buffer_capacity=10000)\n",
    "batch_size = 32\n",
    "num_days = len(dataset)\n",
    "\n",
    "# For each day in the training dataset, train the DDQNAgent\n",
    "for day in range(num_days):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # While still in the day, keep training on that day\n",
    "    while not done and state is not None:\n",
    "        # Select an action based upon Q values returned from MS-CNN\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Calculate what that action does\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay/lookback buffer\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        # Train agent by updating networks to perform better calculations\n",
    "        agent.train(batch_size)\n",
    "\n",
    "        # Update Agent's Position\n",
    "        if action == 1:\n",
    "            agent.position = 'bought_in'\n",
    "        elif action == 2:\n",
    "            agent.position = 'sold_out'\n",
    "\n",
    "        # Increment state and add reward to total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # If training day is done and shares are still being held, sell\n",
    "    if env.shares_held > 0:\n",
    "        last_close_price = env.raw_data[env.current_step + env.window_size - 1, 3]\n",
    "        total_proceeds = env.shares_held * last_close_price\n",
    "        profit = total_proceeds - env.cost_basis\n",
    "        env.cash_balance += total_proceeds\n",
    "        env.shares_held = 0\n",
    "        env.cost_basis = 0.0\n",
    "        env.total_asset_value = env.cash_balance\n",
    "\n",
    "        # Logging\n",
    "        trade_info = {\n",
    "            'datetime': env.datetimes[-1],\n",
    "            'action': 'Sell (EOD)',\n",
    "            'reward': 0.0,\n",
    "            'profit': profit,\n",
    "            'total_asset_value': env.total_asset_value,\n",
    "            'cash_balance': env.cash_balance,\n",
    "            'shares_held': env.shares_held,\n",
    "            'position': 'sold_out'\n",
    "        }\n",
    "        env.trade_log.append(trade_info)\n",
    "\n",
    "        # Update Agent's Position\n",
    "        agent.position = 'sold_out'\n",
    "\n",
    "    # Calculate profit and print training info\n",
    "    total_profit = env.total_asset_value - env.initial_cash\n",
    "    print(f\"Day {day + 1}/{num_days}, Total Reward: {total_reward}, Total Profit: {total_profit}\")\n",
    "\n",
    "    # Convert trade log to csv for each day to see what decisions the bot is making\n",
    "    trade_log = env.trade_log\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"./testing/trade_log_day_{day + 1}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
