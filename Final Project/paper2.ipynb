{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce GTX 1660 Ti\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHLCVDataset(Dataset):\n",
    "    def __init__(self, file_path, window_size=20):\n",
    "        self.window_size = window_size\n",
    "        # Load CSV and extract relevant columns\n",
    "        self.data = pd.read_csv(file_path, parse_dates=[\"datetime\"])\n",
    "        self.raw_data = self.data[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values\n",
    "        # Normalize data\n",
    "        self.normalized_data = self._normalize(self.raw_data)\n",
    "\n",
    "    def _normalize(self, data):\n",
    "        min_vals = data.min(axis=0)\n",
    "        max_vals = data.max(axis=0)\n",
    "        return (data - min_vals) / (max_vals - min_vals + 1e-6)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data) - self.window_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get normalized state\n",
    "        state = self.normalized_data[idx : idx + self.window_size]  # Shape: (window_size, 5)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).transpose(0, 1)  # Shape: (5, window_size)\n",
    "        state_tensor = state_tensor.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 5, window_size)\n",
    "        return state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleScale1D(nn.Module):\n",
    "    def __init__(self, input_channels=5, output_channels=5, kernel_size=3):\n",
    "        super(SingleScale1D, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeByThreeConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=2, kernel_size=3):\n",
    "        super(ThreeByThreeConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveByFiveConv2D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, kernel_size=5):\n",
    "        super(FiveByFiveConv2D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleNet(nn.Module):\n",
    "    def __init__(self, height=5, width=20):\n",
    "        super(MultiScaleNet, self).__init__()\n",
    "        self.single_scale_1d = SingleScale1D(input_channels=5, output_channels=5, kernel_size=3)\n",
    "        self.three_by_three = ThreeByThreeConv2D(input_channels=1, output_channels=2, kernel_size=3)\n",
    "        self.five_by_five = FiveByFiveConv2D(input_channels=1, output_channels=1, kernel_size=5)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, height, width)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Reshape x for SingleScale1D\n",
    "        x_1d = x.view(batch_size, height, width)  # Remove channel dimension: (batch_size, 5, window_size)\n",
    "        features_1d = self.single_scale_1d(x_1d)  # Output shape: (batch_size, 5, new_time_length)\n",
    "\n",
    "        # Pooling and upsampling to match 2D dimensions\n",
    "        features_1d = features_1d.mean(dim=2, keepdim=True)  # Global average pooling along time dimension\n",
    "        features_1d = features_1d.view(batch_size, 5, 1, 1)  # Reshape to (batch_size, 5, 1, 1)\n",
    "        features_1d = F.interpolate(features_1d, size=(height, width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Apply 3x3 and 5x5 Convolutions\n",
    "        features_3x3 = self.three_by_three(x)  # Output shape: (batch_size, 2, height, width)\n",
    "        features_5x5 = self.five_by_five(x)    # Output shape: (batch_size, 1, height, width)\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        combined_features = torch.cat((features_1d, features_3x3, features_5x5), dim=1)\n",
    "        \n",
    "        return combined_features  # Shape: (batch_size, 8, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECABlock(nn.Module):\n",
    "    def __init__(self, channels, gamma=2, b=1):\n",
    "        super(ECABlock, self).__init__()\n",
    "        t = int(abs((math.log(channels, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1  # Ensure k is odd\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        y = self.avg_pool(x)  # (batch_size, channels, 1, 1)\n",
    "        y = y.squeeze(-1).transpose(-1, -2)  # (batch_size, 1, channels)\n",
    "        y = self.conv(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = y.transpose(-1, -2).unsqueeze(-1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, input_channels=8):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.eca = ECABlock(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.eca(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNetWithBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSNetWithBackbone, self).__init__()\n",
    "        self.multi_scale_net = MultiScaleNet()\n",
    "        self.backbone = Backbone(input_channels=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First pass through multi-scale net\n",
    "        x = self.multi_scale_net(x)\n",
    "        # Pass through backbone with attention\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSNetWithQValue(nn.Module):\n",
    "    def __init__(self, num_actions=3):\n",
    "        super(MSNetWithQValue, self).__init__()\n",
    "        self.multi_scale_with_backbone = MSNetWithBackbone()\n",
    "\n",
    "        # Additional Conv3x3 layer before the linear layers\n",
    "        self.conv_final = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Determine the input size for the fully connected layers\n",
    "        sample_input = torch.randn(1, 1, 5, 20)\n",
    "        sample_output = self.multi_scale_with_backbone(sample_input)\n",
    "        sample_output = self.conv_final(sample_output)\n",
    "        feature_map_size = sample_output.view(-1).size(0)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_map_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_scale_with_backbone(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "model = MSNetWithQValue(num_actions=3)\n",
    "test_input = torch.randn(1, 1, 5, 20)\n",
    "output = model(test_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        state_batch = torch.cat(state, dim=0)  # Concatenate along batch dimension\n",
    "        next_state_batch = torch.cat(next_state, dim=0)\n",
    "        return (\n",
    "            state_batch,\n",
    "            torch.tensor(action, dtype=torch.long),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            next_state_batch,\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, buffer_capacity, gamma=0.99, lr=1e-3, target_update_freq=100):\n",
    "        self.actor = MSNetWithQValue()\n",
    "        self.target = MSNetWithQValue()\n",
    "        self.target.load_state_dict(self.actor.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.actor(state)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.buffer.sample(batch_size)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.actor(next_state_batch)\n",
    "            next_actions = next_q_values.argmax(dim=1)\n",
    "            target_q_values = self.target(next_state_batch)\n",
    "            target_q = reward_batch + self.gamma * (1 - done_batch) * target_q_values[range(batch_size), next_actions]\n",
    "\n",
    "        current_q = self.actor(state_batch)[range(batch_size), action_batch]\n",
    "\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnvWithFeatures:\n",
    "    def __init__(self, dataset, k=5):\n",
    "        self.dataset = dataset\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.num_steps = len(dataset)\n",
    "        self.price_history = []\n",
    "        self.initial_cash = 500000  # Starting with $500,000\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.shares_held = 0\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.k = k\n",
    "        self.cost_basis = 0.0\n",
    "\n",
    "        self.raw_close_prices = self.dataset.raw_data[self.dataset.window_size - 1 :, 3]\n",
    "        assert len(self.raw_close_prices) == self.num_steps, \"Length of raw_close_prices does not match num_steps\"\n",
    "        \n",
    "        self.datetimes = self.dataset.data[\"datetime\"].values[self.dataset.window_size - 1 :]\n",
    "        assert len(self.datetimes) == self.num_steps, \"Length of datetimes does not match num_steps\"\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.price_history = []\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.shares_held = 0\n",
    "        self.total_asset_value = self.cash_balance\n",
    "        self.previous_total_asset_value = self.total_asset_value\n",
    "        self.trade_log = []\n",
    "        self.cost_basis = 0.0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        if self.current_step >= self.num_steps:\n",
    "            self.done = True\n",
    "            return None\n",
    "\n",
    "        state = self.dataset[self.current_step]  # Returns the normalized state tensor\n",
    "        current_close_price = self.raw_close_prices[self.current_step]\n",
    "        self.price_history.append(current_close_price)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step + 1 < self.num_steps:\n",
    "            # Get current state and price\n",
    "            state = self.dataset[self.current_step]\n",
    "            current_close_price = self.raw_close_prices[self.current_step]\n",
    "            next_close_price = self.raw_close_prices[self.current_step + 1]\n",
    "            next_state = self.dataset[self.current_step + 1]\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            # Get datetime\n",
    "            datetime = self.datetimes[self.current_step]\n",
    "\n",
    "            # Initialize profit to zero\n",
    "            profit = 0\n",
    "\n",
    "            # Handle actions\n",
    "            if action == 1:  # Buy\n",
    "                if current_close_price == 0:\n",
    "                    num_shares = 0\n",
    "                else:\n",
    "                    num_shares = int(self.cash_balance / current_close_price)\n",
    "                total_cost = num_shares * current_close_price\n",
    "                self.cash_balance -= total_cost\n",
    "                self.shares_held += num_shares\n",
    "                self.cost_basis += total_cost\n",
    "\n",
    "            elif action == 2:  # Sell\n",
    "                num_shares = self.shares_held\n",
    "                total_proceeds = num_shares * current_close_price\n",
    "                self.cash_balance += total_proceeds\n",
    "                self.shares_held = 0\n",
    "                profit = total_proceeds - self.cost_basis  # Assuming you track cost basis\n",
    "                self.cost_basis = 0\n",
    "            \n",
    "            else:\n",
    "                profit = 0.0\n",
    "\n",
    "            # Update total asset value\n",
    "            self.previous_total_asset_value = self.total_asset_value\n",
    "            self.total_asset_value = self.cash_balance + self.shares_held * current_close_price\n",
    "\n",
    "            # Compute profit as change in total asset value\n",
    "            profit = self.total_asset_value - self.previous_total_asset_value\n",
    "\n",
    "            # Compute reward\n",
    "            reward = self.calculate_reward(action, current_close_price)\n",
    "\n",
    "            action_mapping = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
    "            # Log the trade\n",
    "            trade_info = {\n",
    "                'datetime': datetime,\n",
    "                'action': action_mapping[action],\n",
    "                'reward': reward,\n",
    "                'profit': profit,\n",
    "                'total_asset_value': self.total_asset_value,\n",
    "                'cash_balance': self.cash_balance,\n",
    "                'shares_held': self.shares_held\n",
    "            }\n",
    "            self.trade_log.append(trade_info)\n",
    "\n",
    "            self.current_step += 1\n",
    "            done = self.current_step >= self.num_steps - 1\n",
    "            return next_state, reward, done, {}\n",
    "        else:\n",
    "            self.done = True\n",
    "            return None, 0, True, {}\n",
    "\n",
    "    def calculate_reward(self, action, current_close_price):\n",
    "        \"\"\"\n",
    "        Calculates the reward based on the agent's action and future price movements.\n",
    "        Implements the short-term Sharpe ratio as the reward function.\n",
    "        \"\"\"\n",
    "        # Determine POS_t\n",
    "        POS_t = 1 if self.shares_held > 0 else 0\n",
    "\n",
    "        # Compute R_k_t\n",
    "        R_k_t = []\n",
    "        for i in range(1, self.k + 1):\n",
    "            future_step = self.current_step + i\n",
    "            if future_step >= self.num_steps:\n",
    "                break\n",
    "            future_price = self.raw_close_prices[future_step]\n",
    "            if current_close_price == 0:\n",
    "                R_k_t_i = 0  # Avoid division by zero\n",
    "            else:\n",
    "                R_k_t_i = (future_price - current_close_price) / current_close_price\n",
    "            R_k_t.append(R_k_t_i)\n",
    "\n",
    "        if len(R_k_t) < 2:\n",
    "            # Cannot compute Sharpe ratio with less than 2 data points\n",
    "            SR_t = 0\n",
    "        else:\n",
    "            mean_R = np.mean(R_k_t)\n",
    "            std_R = np.std(R_k_t)\n",
    "            if std_R == 0:\n",
    "                SR_t = 0  # Avoid division by zero\n",
    "            else:\n",
    "                SR_t = mean_R / std_R\n",
    "\n",
    "        SSR_t = POS_t * SR_t\n",
    "\n",
    "        return SSR_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     18\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     22\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[72], line 38\u001b[0m, in \u001b[0;36mDDQNAgent.train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m     target_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget(next_state_batch)\n\u001b[0;32m     36\u001b[0m     target_q \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done_batch) \u001b[38;5;241m*\u001b[39m target_q_values[\u001b[38;5;28mrange\u001b[39m(batch_size), next_actions]\n\u001b[1;32m---> 38\u001b[0m current_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mrange\u001b[39m(batch_size), action_batch]\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(current_q, target_q)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[69], line 21\u001b[0m, in \u001b[0;36mMSNetWithQValue.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_scale_with_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_final(x)\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[68], line 11\u001b[0m, in \u001b[0;36mMSNetWithBackbone.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_scale_net(x)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Pass through backbone with attention\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[67], line 15\u001b[0m, in \u001b[0;36mBackbone.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\codyb\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = \"./labeling/TSLA_minute_data_cleaned_labeled.csv\"\n",
    "dataset = OHLCVDataset(file_path)\n",
    "env = StockTradingEnvWithFeatures(dataset)\n",
    "agent = DDQNAgent(buffer_capacity=10000)\n",
    "\n",
    "num_episodes = 100\n",
    "batch_size = 32\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done and state is not None:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        agent.train(batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Update total_asset_value at the end of the episode\n",
    "    if env.shares_held > 0:\n",
    "        last_close_price = env.raw_close_prices[env.current_step - 1]\n",
    "        env.total_asset_value = env.cash_balance + env.shares_held * last_close_price\n",
    "\n",
    "    total_profit = env.total_asset_value - env.initial_cash\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Total Profit: {total_profit}\")\n",
    "\n",
    "    # Access and process the trade log\n",
    "    trade_log = env.trade_log\n",
    "\n",
    "    # Optionally, save the trade log to a CSV file\n",
    "    trade_log_df = pd.DataFrame(trade_log)\n",
    "    trade_log_df.to_csv(f\"trade_log_episode_{episode + 1}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
